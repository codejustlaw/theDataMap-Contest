{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Data Flow Analysis\n",
    "\n",
    "### By: Ritika Sharma & Andres Y. Gonzalez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- 1. Overview\n",
    "- 2. Problem \n",
    "- 3. General Questions\n",
    "- 4. Data Gathering\n",
    "    - a. Quick Overview of Data Attributes \n",
    "- 5. Data Cleaning and Normalizing \n",
    "- 6. Results\n",
    "    - a. Visual Analysis through Graph Plotting \n",
    "        - (i) Histograms \n",
    "        - (ii) Heat Maps\n",
    "    - b. Data Prediction and Modeling\n",
    "- 7. Challenges\n",
    "- 8. Future work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "\n",
    "In this project, we decided to use a health data set available at  [TheDataMap.org](https://thedatamap.org).  TheDataMap is an online portal for documenting flows of personal data, which tells you where your data goes. \n",
    "\n",
    "Our goal was to produce a detailed description of personal data flows in the United States. The project is not limited to health data, rather it includes the full spectrum of sharing personal information. While this project started back in 2010, the data collected ranges from the years 2005-2018. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Problem \n",
    "\n",
    "There is more and more data available out there everyday; health data is particularly sensitive because, in addition to identifiable information such as one’s social security, insurance, or credit card number,  it can also reveals an individual’s most intimate secrets.\n",
    "\n",
    "While there are laws such as [HIPAA](https://www.hhs.gov/hipaa/for-individuals/faq/index.html) (The Health Insurance Portability and Accountability Act of 1996), which in this case provides data privacy and security provisions for safeguarding medical information. These laws do not adequately address issues regarding increasing cyber threats and data availability. \n",
    "\n",
    "##### Why did we pick this data set? \n",
    "\n",
    "Similarly to the folks at datamap.org, our goal is to our data science skills to advance datamap’s mission “to help journalists, advocates, regulators, policy makers and researchers understand the current state of personal data sharing so they can do their jobs better.” Additionally, this project allows us to combine our knowledge and professional backgrounds.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Questions to Analyze \n",
    "\n",
    "- Which state had the most data breaches?\n",
    "- Which type had the most data breaches?\n",
    "- Which category had the most data breaches?\n",
    "- Which entity shared the most data? \n",
    "- Through which sources are people informed about breaches? \n",
    "- What type of data was breached the most? \n",
    "- Can we use some prediction models to make predictions about the future breach incidents?\n",
    "- How can we use different prediction models to predict some or the other attributes based on the certain information of some attributes?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Gathering\n",
    "\n",
    "The data we collected from theDataMap provides information about health data flows in the United States.\n",
    "\n",
    "On analyzing this data, we can get an idea about the points such as how the data flows, which departments make the most use of this information, and how could this information be used in a better way so that individuals understand how their personal information could benefit businesses. \n",
    "\n",
    "### The files used in this analysis are: \n",
    "\n",
    "- #### orgsindex.csv \n",
    "is a list of organizations and entities whose data sharing transaction(s) appear on theDataMap. \n",
    "\n",
    "- #### categories.csv \n",
    "is a list of categories of data holders of health data.\n",
    "\n",
    "- #### catsorgs.csv \n",
    "is an association list of categories (CatID) from the categories file and organizations (OrgID) from the OrgsIndex file.\n",
    "\n",
    "- #### prcbreaches2005-18.csv \n",
    "contains a list of breaches associated with different categories and organizations.\n",
    "\n",
    "- #### edges.xlsx \n",
    "contains paths/directions of data transfer from one category to another in order to form the edges seen on the current visualization for the health data.\n",
    "\n",
    "- #### categories_info.xlsx \n",
    "contains the names of each category, with description for each. We did not use this file anywhere in our code but we did use it as a reference for our analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Quick Overview of Data Attributes \n",
    "\n",
    "Before we proceed, let's take a quick look to understand some of the attributes present in this data set. \n",
    "\n",
    "- Records.Breached - The total number of records breached for some of the organizations.\n",
    "- Records.Breached...Detail - The number of records breached with more details about the breaches.\n",
    "- Name - The name of the organization.\n",
    "- CatID - The category ID used in the other data files.\n",
    "- Total.Records - The total number of breaches for all of the organizations. \n",
    "- Region - The region that the organization is located in.\n",
    "- Contact..etc. - The contact information for the organization. \n",
    "- Category_dm - The name of the category associated with the CatID. (Physician, Payer, etc.)\n",
    "- Entity_prc - The type of organization. (MED, BSO, EDU, NGO, GOV, BSF, BSR)\n",
    "- State - The state that the organization is located in.\n",
    "- OrgID - The Organization ID.\n",
    "- Location - The city that the organization is located in.\n",
    "- Date.Made.Public - The date that the breaches were made public.\n",
    "- Year - The year that the breaches were made public.\n",
    "- Source.of.Breach.Notification - The source that publicized the breaches.\n",
    "- Type - The type of breach. (CARD, UNKN, STAT, INSD, PORT, PHYS, DISC, HACK)\n",
    "- Description - The description of the breach.\n",
    "- Example (HTML) - The HTML code used to generate the list of examples for each category in the health visualization.\n",
    "- FromCatID - the starting point of the edge.\n",
    "- ToCatID - The ending point of the edge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Cleaning and Normalizing\n",
    "\n",
    "The data we have, needed to be cleaned as there was a lot of information that was missing from the data set. Some of the column information was redundant and of no use. After we finished cleaning our data, we started to plot so we could find the answer to our preliminary questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the required imports\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import matplotlib\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import dateutil.parser\n",
    "from sklearn.datasets import load_iris\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads a csv file ignoring the first header row\n",
    "# convert all fields to int\n",
    "# returns a pandas dataframe\n",
    "def readCSV(fname):\n",
    "    hdr=[]\n",
    "    with open(fname, newline='', encoding = \"ISO-8859-1\") as csvfile:\n",
    "        csvdata = csv.reader(csvfile, delimiter=',')\n",
    "        i=0\n",
    "        data=[]\n",
    "        for row in (csvdata):\n",
    "            if i>0:\n",
    "                data.append([col for col in row])\n",
    "            elif i==0:\n",
    "                hdr=row\n",
    "                print([x for x in enumerate(hdr)])\n",
    "            i = i+1\n",
    "    return hdr, pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'CatID'), (1, 'CatName'), (2, 'Coordinates'), (3, 'Hover')]\n",
      "RangeIndex(start=0, stop=4, step=1)\n"
     ]
    }
   ],
   "source": [
    "#reading the categories.csv\n",
    "\n",
    "hdr, df_categories = readCSV('categories.csv')\n",
    "print(df_categories.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'Records.Breached'), (1, 'Records.Breached...Detail'), (2, 'Name'), (3, 'CatID'), (4, 'Total.Records'), (5, 'Region'), (6, 'Contact..etc.'), (7, 'Category_dm'), (8, 'Entity_prc'), (9, 'State'), (10, 'OrgID'), (11, 'Location'), (12, 'Date.Made.Public'), (13, 'Year'), (14, 'Source.of.Breach.Notification'), (15, 'Type'), (16, 'Description'), (17, 'Example (HTML)'), (18, '')]\n"
     ]
    }
   ],
   "source": [
    "#reading the prcbreaches2005-18.csv\n",
    "hdr, df_prcbreaches = readCSV('prcbreaches2005-18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column names\n",
    "df_prcbreaches.columns\n",
    "df_prcbreaches.columns = [\"Records.Breached\", \"Records.Breached.Detail\", \"Name\", \"CatID\",\"Total.Records\", \"Region\",\"Contact.etc\",\"Category_Name\", \"Entity_prc\", \"State\",\"OrgID\", \"Location\", \"Date.Made.Public\",\"Year\",\"Source.of.Breach.Notification\",\"Type\",\"Description\",\"Example(HTML)\",\"NA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'CatID'), (1, 'OrgID'), (2, 'Type')]\n"
     ]
    }
   ],
   "source": [
    "#reading the catsorgs.csv\n",
    "hdr, df_catsorgs = readCSV('catsorgs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'OrgID'), (1, 'Name'), (2, 'SourceType')]\n"
     ]
    }
   ],
   "source": [
    "#reading the orgsindex.csv \n",
    "hdr, df_orgs_index = readCSV('orgsindex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'FromCatID'), (1, 'ToCatID'), (2, 'NoName'), (3, ''), (4, '')]\n"
     ]
    }
   ],
   "source": [
    "# We first manually converted the edges.xlsx to edges.csv to use it in pandas dataframe\n",
    "# Now, reading the edges.csv\n",
    "\n",
    "hdr, df_fromCat_toCat = readCSV('edges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.a From prc_breaches:  \n",
    "\n",
    "There were certain columns that contained redundant information like the Records Breached and Total.Records.\n",
    "\n",
    "Other columns such as breach description Contact Info : either did not add much information to the set of questions we planned to analyse or had most of the column values missing, hence we decided to drop these columns.\n",
    "\n",
    "The other challenge was replacing missing values from the columns that were important, since most of the data is categorical we chose to replace these with a string value ‘NA’.\n",
    "\n",
    "For several entries the total breach record count was also not recorded that was another challenge in this data set, hence we planned to either replace it with a zero or any other numeric value.\n",
    "\n",
    "Replacement with zero was not a feasible solution as Total Record column with zero would denote that there were no records that were breached. Hence we chose to replace those empty column values with the mean of the total breach records of all the other data entries.\n",
    "\n",
    "We also thought to process the Date of Breach column values. The date entries had different formats so we extracted the Month_of_Breach value for each record from this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are creating a replica of the databreaches data set so that all the cleaning is done in the replica rather than the original data frame values\n",
    "#renaming column names for df_prcbreaches\n",
    "\n",
    "df_prcbreaches_temp = df_prcbreaches\n",
    "#removing unuseful columns from the data frame\n",
    "df_prcbreaches_temp = df_prcbreaches_temp.drop(columns=['Records.Breached','Records.Breached.Detail','Contact.etc','Description','Example(HTML)','NA']) \n",
    "#df_prcbreaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method tests if any of the column value is missing for each row\"\"\"\n",
    "def testifcolhasmissingvalues(colstart, colend):\n",
    "    for i in range(len(df_prcbreaches_temp.iloc[: ,int(colstart):int(colend)])):\n",
    "        j = i+1\n",
    "        if (df_prcbreaches_temp.iloc[i:j,int(colstart):int(colend)] == '').bool():\n",
    "            print(\"Yes true has missing data\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"On calculating using 'testifcolhasmissingvalues' method, we get the following columns that contained missing information\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"On calculating using 'testifcolhasmissingvalues' method, we get the following columns that contained missing information\"\"\"\n",
    "#data missing for location\n",
    "#no data missing for year AND duration\n",
    "#testifcolhasmissingvalues(13,14)\n",
    "#data missing for source of breach column\n",
    "#no missing data for type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method replaces blank column values from df_breach_temp data frame with 'NA'\n",
    "\"\"\"\n",
    "def putNAWhereRecordValueisMissing(colstart, colend):\n",
    "    count = 0\n",
    "    for i in range(len(df_prcbreaches_temp.iloc[: ,int(colstart):int(colend)])):\n",
    "        #print(type(i))\n",
    "        j = i+1\n",
    "      \n",
    "        if (df_prcbreaches_temp.iloc[i:j,int(colstart):int(colend)] == '').bool():\n",
    "            df_prcbreaches_temp.loc[int(i):int(j),int(colstart):int(colend)] = 'NA'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We need to add NA for these columns as they have missing values\"\"\"\n",
    "putNAWhereRecordValueisMissing(3,4)\n",
    "putNAWhereRecordValueisMissing(6,7)\n",
    "putNAWhereRecordValueisMissing(8,9)\n",
    "putNAWhereRecordValueisMissing(11, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We are placing a 111 for places where the value is missing in Total Records column. \n",
    "We are not using a 0 because there are columns that already have a 0, \n",
    "so to distinguish these columns for which the count is not known we are using a -1 value\"\"\"\n",
    "def putUniqueNumberWhereTotalRecordValueisMissing(colstart, colend):\n",
    "    count = 0\n",
    "    for i in range(len(df_prcbreaches_temp.iloc[: ,int(colstart):int(colend)])):\n",
    "        #print(type(i))\n",
    "        j = i+1\n",
    "      \n",
    "        if (df_prcbreaches_temp.iloc[i:j,int(colstart):int(colend)] == '').bool():\n",
    "            df_prcbreaches_temp.loc[int(i):int(j),int(colstart):int(colend)] = '111'\n",
    "\n",
    "#only the third column requires this method call            \n",
    "putUniqueNumberWhereTotalRecordValueisMissing(2,3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prcbreaches_temp[\"Total.Records\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the columns with had actual breached records info to calculate the mean\n",
    "df_TR_mean_cal= df_prcbreaches_temp[df_prcbreaches_temp[\"Total.Records\"] != 111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the column values from Total Records with numeric values by removing the ',' present in the data values \n",
    "df_prcbreaches_temp[\"Total.Records\"] = df_prcbreaches_temp[\"Total.Records\"].str.replace(\",\",\"\").astype(float)\n",
    "df_prcbreaches_temp[\"Total.Records\"] = df_prcbreaches_temp[\"Total.Records\"].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating mean of the total records to replace the missing column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "mean_total_rec = df_TR_mean_cal.mean()\n",
    "print(mean_total_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e974158612c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# replacing the places with missing values with the mean value of the total breaches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_prcbreaches_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Total.Records\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_prcbreaches_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Total.Records\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_total_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#df_prcbreaches_temp[\"Total.Records\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         raise TypeError(\"cannot convert the series to \"\n\u001b[0;32m--> 117\u001b[0;31m                         \"{0}\".format(str(converter)))\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "# replacing the places with missing values with the mean value of the total breaches\n",
    "df_prcbreaches_temp[\"Total.Records\"] = df_prcbreaches_temp[\"Total.Records\"].replace(111, int(mean_total_rec))\n",
    "#df_prcbreaches_temp[\"Total.Records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prcbreaches_temp[\"Total.Records\"] = df_prcbreaches_temp[\"Total.Records\"].astype('int64')\n",
    "df_prcbreaches_temp[\"Year\"] = df_prcbreaches_temp[\"Year\"].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting more information from the dataframe columns so that we can use it better for analysis using month of breach or discharge\n",
    "df_prcbreaches_temp['Month.of.Breach']=''\n",
    "def addMonth(colstart, colend, col1,col2):\n",
    "    i = 0\n",
    "    month_list = []\n",
    "    for item in (df_prcbreaches_temp.iloc[: ,int(colstart):int(colend)]['Date.Made.Public'].tolist()):\n",
    "        date = dateutil.parser.parse((item))\n",
    "        #print(dateutil.parser.parse((item)).month)\n",
    "        month = dateutil.parser.parse((item)).month\n",
    "        month_list.append(month)\n",
    "        j = i + 1\n",
    "        #print(month)\n",
    "        df_prcbreaches_temp.loc[int(i),'Month.of.Breach'] = int(month_list[i])\n",
    "        i = i+1\n",
    "\n",
    "#only the third column requires this method call            \n",
    "addMonth(9,10,14,15)   \n",
    "# df_prcbreaches_temp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.b From categories.csv: \n",
    "\n",
    "We dropped the Coordinates and Hover columns, as they were not relevant to our questions. \n",
    "Additionally, we added a column ‘BreachorData’ to assign numeric notation as 1 for Breaches and 0 For Discharge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering categories.csv first\n",
    "# rename column names to original column names\n",
    "df_categories.columns = [\"CatID\", \"CatName\", \"Coordinates\", \"Hover\"]\n",
    "print(df_categories.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.c From edges.csv: \n",
    "\n",
    "The only information that we had was The flow of information from one department to the other.\n",
    "\n",
    "Here we have removed some blank columns that added nothing to our data, and also we added two columns that provide information about the category Name corresponding to each category Id with respect to FromCatId and ToCatID columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_catsorgs_temp= df_catsorgs\n",
    "# #organisation_info = organisation_info.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the edges table\n",
    "df_fromCat_toCat.columns=[\"FromCatID\",\"ToCatID\",\"A\",\"B\",\"C\"]\n",
    "df_fromCat_toCat[\"Count\"] = \"0\"\n",
    "df_fromCat_toCat = df_fromCat_toCat.drop(columns=['A','B','C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of category labels\n",
    "# we need this to add labels corresponding to toCatId and fromCatId in the edges table for better understanding of the flow\n",
    "category_dic = {1:'Prescription Analytics', \n",
    "                2:'Pharmaceutical Company',\n",
    "                3:'Pharmacy',\n",
    "                4:'Pharmacy Benefits Manager',5:'Clearing House',\n",
    "                6:'Life Insurance Company',\n",
    "                7:'Vital Statistics',\n",
    "                8:'Transcription',\n",
    "                9:'Coding',\n",
    "                10:'ICU Management',\n",
    "                11:'Employers Wellness Program',\n",
    "                13:'You, the Patient',\n",
    "                14:'Medical Devices',\n",
    "                15:'Employer',\n",
    "                16:'Consulting Physician',\n",
    "                17:'Payer (Insurer)',\n",
    "                18:'Physician, Hospital',\n",
    "                19:'Accreditation',\n",
    "                21:'De-identification',\n",
    "                22:'Law Firms',\n",
    "                23:'Public Health',\n",
    "                24:'Clinical Lab',\n",
    "                25:'Analytics',\n",
    "                26:'Disease Management',\n",
    "                27:'Personal Health Record',\n",
    "                28:'Discharge Data',\n",
    "                29:'Researcher',\n",
    "                30:'CDC',\n",
    "                31:'Online Websites',\n",
    "                33:'Employee Union',               \n",
    "                34:'Health IT',\n",
    "                35:'Other Government',\n",
    "                36:'Federal Trade Commission',\n",
    "                37:'Financial',\n",
    "                38:'Media',\n",
    "                39:'Real Estate',\n",
    "                40:'Blood & Tissue',\n",
    "                41:'Retirement & Disability',\n",
    "                42:'Human Resources',\n",
    "                43:'SSA',\n",
    "                44:'Personal Transport',\n",
    "                45:'Mental & Addiction',\n",
    "                46:'Dental and Vision',\n",
    "                47:'Home Health',\n",
    "                48:'Care Facility',\n",
    "                49:'Debt Collection',\n",
    "                50:'Law & Justice',\n",
    "                51:'Social Services',\n",
    "                52:'Education',                \n",
    "                53:'Copy&Transport',\n",
    "                54:'Registries',\n",
    "                55:'Associations',\n",
    "                56:'Licensing',\n",
    "                57:'Social Support'}\n",
    "# adding the label columns to store corresponding category_name\n",
    "df_fromCat_toCat[\"FromCat_label\"] = \"\"\n",
    "df_fromCat_toCat[\"ToCat_label\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the label values to corresponding category Ids\n",
    "def addCategoryLabelColumnForEachRow(category_dic, columnName, labelName):\n",
    "    value = ''\n",
    "    j = 0\n",
    "    for i in df_fromCat_toCat[columnName]:\n",
    "        df_fromCat_toCat.loc[int(j)][labelName] = category_dic.get(int(i))\n",
    "        j +=1\n",
    "\n",
    "\n",
    "addCategoryLabelColumnForEachRow(category_dic, 'FromCatID', 'FromCat_label')  \n",
    "addCategoryLabelColumnForEachRow(category_dic, 'ToCatID', 'ToCat_label')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6(a)(i). Visualization using histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We use this method to plot histograms the histograms will appear in sorted order\"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "def plot_hist(data_list):\n",
    "    c = list(Counter(data_list).items())\n",
    "    c.sort(key = lambda item: item[1])\n",
    "    labels, values = zip(*c)\n",
    "    width = 1\n",
    "    indexes = np.arange(len(labels))\n",
    "    plt.bar(indexes,values,width)\n",
    "    plt.xticks(indexes + width * 0.5, labels)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(i)(1). Total Number of Breaches per Category\n",
    "\n",
    "- This graph allows to see the distribution of breaches per category. This was one of the first graphs we plotted hoping not only to gain new insights but also to see if there were any red flags. None of the categories we saw with the highest incidents were surprising to us. Discharge data being at the top was predictable because these files often contain a comprehensive overview of the reason a patient visit the hospital and in most cases it contains additional informationa such as medical history, insurance number or social security number, address, old and new prescriptions and much more. Because this type of data can be quite comprehensive, it often becomes a desirable target for hackers.\n",
    "\n",
    "- The transition to electronic medical records has also contributed to the availability of this type of data. Physician and Hospital and Insurance companies are the top entities receiving and releasing health data (which we will show with our plotting for the CatFrom and CatTo attributes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(df_prcbreaches_temp.iloc[:,4:5]['Category_Name'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(i)(2). Total of Breaches per State \n",
    "\n",
    "- We plotted this graph to further our understanding about the health data flow in the US. This graph shows California, Texas, Maryland, and New York among the states with highest incidents. \n",
    "\n",
    "- One of the intersting things we can infer from this graph is the impact that state laws have on the number of data breaches reported. Additionally, we see the correlation between highly populated states and number of breaches. We were not alarmed to see CA at the top. Not only is CA a densely populated state, but it was the first state to pass the breach notification law. \n",
    "\n",
    "- It is also not surprising that CA is one of only a few states where the word \"privacy\" can be found in the state's constitution. CA has also taken pride in protecting the privacy of its residents and even today we see CA as a pioner on the privacy and data security front. An example of this is the recent passing of the California Consumer Protection Act of 2018. This act mirrors the GDPR, which is the new European data privacy law, and is thought to be the most encompassing data privacy protection in the US thus far. Some legislators have been calling Congress to take action and are advocating for a federal privacy legislation. \n",
    "\n",
    "- If a federal privacy law passes, it is likely that it will not preempt HIPAA. But even if that is the case, the healthcare industry will see the effect of a new privacy law that can regulate entities which today are not required to abide by HIPAA. If that is the case, we can expect to see a great increase on the number of breaches reported in future years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plot_hist(df_prcbreaches_temp.iloc[:,6:7]['State'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick look into the States and Years with maximum breaches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of top 10 states with least data breaches\n",
    "\n",
    "c = (list(Counter(df_prcbreaches_temp.iloc[:,6:7]['State'].tolist()).items()))\n",
    "c.sort(key = lambda item: item[1])\n",
    "print(c[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the years with maximum breaches\n",
    "c = (list(Counter(df_prcbreaches_temp.iloc[:,10:11]['Year'].tolist()).items()))\n",
    "c.sort(key = lambda item: item[1])\n",
    "print(c[1:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(i)(3). Number of Breaches v. Type of Breach\n",
    "\n",
    "#### We decided to break down the total number of breaches into \"types\" \n",
    "- Following the literature we've reviewed we were expecting the highest number to be\"HACK\"\n",
    "\n",
    "- This result is also not unexpected given that most news stories about data breaches are hack-related. However, there was a chance the news were associating most data breaches with HACK(s). What is covered by the news becomes relevant in the case of health data because of the HIPAA's [notification rule](https://www.hhs.gov/hipaa/for-professionals/breach-notification/breach-reporting/index.html), which requires breaches that expose unencrypted health data of 500+ patients/users to be reported to the media.\n",
    "\n",
    "- We were not surprised to see DISC and PHYS also at the top. PHYS refers to the cases when, for instance, someone makes a mistake that inadvertedly exposes the health information of patients. An example of that is the [Aetna case](https://www.statnews.com/2017/08/24/aetna-hiv-envelopes/), where an entity exposed the HIV status of patients through an envelope window. DISC refers to breaches involving discharged data. Even though discharge is processed so it does not identify individuals once shared, sophisticated de-cryption methods as well the increasing availability of data have lead to a higher number of breaches involving discharge data. \n",
    "\n",
    "- A entity covered by HIPAA must use one of two methods (expert determination and safe harbor, which includes the removal of 18 type of identifiers) to encrypt patient health information, whether stored or in transit. For more information, please see point [1.3](https://www.hhs.gov/hipaa/for-professionals/breach-notification/breach-reporting/index.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.hist(df_prcbreaches_temp.iloc[:,12:13]['Type'].tolist(), bins=20)\n",
    "# df_breach_temp.iloc[:50,13:14]['Category Name'].hist(bins =3)\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(i)(4). Number of breaches per year from 2005-2018. \n",
    "\n",
    "- Next, we plotted a graph to see the distribution of breaches per year. Our data set is limited to breaches that have occurred between 2005-2018. \n",
    "\n",
    "- We were expecting to see an increase in reported breaches after 2013. Hence why we are not alarmed years 2014-2016 were at the top. The reasons that was our expectation are (1) because states were starting to adopt breach notification rules, and (2) the HITECH rule passed in 2013 and modified HIPAA. \n",
    "\n",
    "- This modification extended liability to business associates or third parties that before did not have a duty to report. \n",
    "\n",
    "- California was the first state to pass a breach notification rule in 2003, even before it was mandated by HIPAA. Since its inception, HIPAA has provided a minimun standard for states to follow allowing them to adopt stricter rules. Many states followed CA and also passed their own notification rules. \n",
    "\n",
    "- We were at first alarmed to see 2018 being much lower than the three years prior; however, we soon realized the data we have for 2018 only goes until May 2018. So if the information is collected in the same way it was over the past few years, we do not expect that number of breaches in 2018 to be much different from the previous years. \n",
    "\n",
    "- The passing of a new federal privacy regulation may cause a drastic change if reporting is expanded to many entities that deal with healh information yet are not covered under HIPAA. Also, a federal privacy law may push forward a stricter breach notification law. For instance, instead of mandatory reporting to the media in breaches that affect over 500 it could make it so for all breaches. Also, improved ways to determine when a breach has occurred may change these numbers as well in the forseable future. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plot_hist(df_prcbreaches_temp.iloc[:,10:11]['Year'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(i)(5). Total Number of Breaches per Entity \n",
    "\n",
    "- Not only health related entities handle health data. This graph represents a distribution of health data breached by entities. Our dataset grouped those entities in various categories. For instance, MED refers to medical, BSO refers to business, GOV refers to goverment, NGO refers to non-profits, and EDU refers to education (universities, high schools, etc.) \n",
    "\n",
    "- Our expectation was confirmed by our finding showing most health data breached comes from health (MED) entities. This category would include hospital, clinics, labs, etc. Because of the way our healthcare system works, insurance companies are heavily involved in the handling of health data in order to process payments, bill patients, etc. Therefore, seeing BSO category, which include insurance companies on the second place was also not surprising to us. \n",
    "\n",
    "- An interesting follow-up project may seek to break down the \"MED\" category into separate attributes. A similar visualization can be made by looking at the CatTo and CatFrom attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "#plt.xticks(getMonthLabel(top_7_months))\n",
    "plot_hist(df_prcbreaches_temp.iloc[:,5:6]['Entity_prc'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(i)(6). Total Number of Breaches v. Reporting Entity \n",
    "\n",
    "#### Next, we wanted to find out which source is playing a bigger role in keeping individuals informed about data breaches. \n",
    "\n",
    "- We learned that US Dept of Health and Human Services was number one, which we expected because the Department of Health and Human Services (DHHS) is the enforcement agency for HIPAA and entities that are breached are required to update DHHS. \n",
    "\n",
    "- Similarly, we were not surprised to see the media in second place. As we previously mentioned, the HITECH rule requires health data breaches affecting 500+ individuals to be reported to the media. Because of that limitation, even though lately it feels the media is constantly reporting data breaches, we were not expecting the media to be number one. The reason is there are many small breaches (those affecting less than 500 people) that entities are not required to report to the media, yet they would have to report to the DHHS and inform users/patients.\n",
    "\n",
    "- As per the California and Maryland AGs, it made sense they are also among the top sources. CA was the first state to pass a breach notification rule, which requires breached entities to informed consumers and report the breach to the AG if certain criteria is met. Maryland followed california and so did all the other states. The last two states to pass a breach notification law did it last year. \n",
    "\n",
    "- Since all states have mandates to report, we expect to see more the states with the most number of breaches to also be the states with the highest number of notifications. The media and DHHS will likely continue to be at the top since they pertain to all states. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plot_hist(df_prcbreaches_temp.iloc[:,11:12]['Source.of.Breach.Notification'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6(a)(ii). Visualization using heatmaps\n",
    "\n",
    "Here we are plotting some heatmaps to get better view of the relationship between the attributes and compare to what we learn from the histograms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(ii)(1). Category v. Year of Breach\n",
    "\n",
    "Firstly, we want to get an overview of how breaches are distributed in terms of year and the categories of the businesses involved. The following heatmap shows the intensity of the breaches grouped by categories and year.\n",
    "\n",
    "This graph correlated with some of what we saw on our histograms. The most noticeable feature here can be seen in terms of the discharge data category. A closer look at that category shows year 2014 with the darkest orange color. As we mentioned above, we expected to see an increase on breaches reported after 2013 when the HITECH law passed. It does not necessarily means there were more breaches or that companies were more or less vulnerable to cyber threats. Rather, the most accurate interpretation reveals the influence that the new law (as it relates to breach notification) may have had on the rising number of breach incidents reported from 2013 to 2014. As a reminder, HITECH not only added the breach notification to HIPAA but it also expanded liability to third parties that were not previously covered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(50,15)})\n",
    "a = df_prcbreaches_temp.groupby(['Category_Name', 'Year']).count()\n",
    "agg_data = a.add_suffix('_Count').reset_index()\n",
    "ha = agg_data[['Category_Name', 'Year', 'Name_Count']]\n",
    "ha = ha.pivot('Year', 'Category_Name', 'Name_Count')\n",
    "ha.fillna(0)\n",
    "sns.heatmap(ha, cmap='Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** The intensity of color in the above heatmap relates to the number of breach associated with a category for a single year. Every rectangular block has an intensity of the blue color, higher intensity refers to higher number of breaches and lower intensity refers to lower number of breaches. As we can see, there is high intensity for the `Discharge Data` in the years `2014`, `2015`, `2016` and `2017`. The columns `Payer (Insurer)` and `Physician, Hospital` also have similar streak of higher intensity. Intrestingly, the `Public Health` column has higher intensity for the year `2016`. There might have occured some severe security breaches in `2016` in the `Public Health` sector. A quick google showed [related result](https://www.healthcare-informatics.com/news-item/cybersecurity/report-healthcare-data-breaches-hit-all-time-high-2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(ii)(2). Year of Breach v. State \n",
    "\n",
    "Another heap map that we are plotting here shows the breach intensity for a year within a US state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(50,10)})\n",
    "a = df_prcbreaches_temp.groupby(['State', 'Year']).count()\n",
    "agg_data = a.add_suffix('_Count').reset_index()\n",
    "ha = agg_data[['State', 'Year', 'Name_Count']]\n",
    "ha = ha.pivot('Year', 'State', 'Name_Count')\n",
    "ha.fillna(0)\n",
    "sns.heatmap(ha, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis** We see that California is the State that has maximum number of breach incidences followed by Maryland. Although the breaches were consistently high in California, but Maryland experienced the maximum breach incidents in the year `2014` where with some reading we found some information [maryland_breach](https://healthitsecurity.com/news/maryland-court-dismisses-carefirst-data-breach-lawsuit) that provided us some interesting facts about the incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(ii)(2). **`Type` of Breach v. `Year`**\n",
    "To get an overview of how types of data breaches are distributed according to year, we will be plotting heatmap with `Year` and `Type` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(8,6)})\n",
    "a = df_prcbreaches_temp.groupby(['Type', 'Year']).count()\n",
    "agg_data = a.add_suffix('_Count').reset_index()\n",
    "ha = agg_data[['Type', 'Year', 'Name_Count']]\n",
    "ha = ha.pivot('Year', 'Type', 'Name_Count')\n",
    "ha.fillna(0)\n",
    "sns.heatmap(ha, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that `HACK` records are kind of consistent across the years with exception for year 2014. Still `HACK` is the major breach `Type` for the year `2014`. There are more breaches for `PHYS` in year 2013 compared to other years for `PHYS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(ii)(3). Source of Breach Notification v. State\n",
    "\n",
    "Analyzing the Source of breach notification's role in each State through a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(16,6)})\n",
    "a = df_prcbreaches_temp.groupby(['Source.of.Breach.Notification', 'State']).count()\n",
    "agg_data = a.add_suffix('_Count').reset_index()\n",
    "ha = agg_data[['Source.of.Breach.Notification', 'State', 'Name_Count']]\n",
    "ha = ha.pivot('Source.of.Breach.Notification', 'State', 'Name_Count')\n",
    "ha.fillna(0)\n",
    "sns.heatmap(ha, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Analysis*** Here we see that `California Attorney General` and `Maryland Attorney General` were the source that notified maximum breach incidents in the respective states. We also observed that `US Health and Human Services` department was another key source in `California` through which these incidents got reported.\n",
    "\n",
    "We can also not neglect the role of `Media` in reporting about such incidents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(ii)(4). CatFrom v. CatTo (Health Data Flow between Categories)\n",
    "\n",
    "We have used the `edges` table to draw a cool heat map that makes us easier to understand the flow of information from one category department to the other. Sections or small sublocks that are not colored indicate that no information has been exchanged between those departments. We can look for the dark shaded blocks to find the categories that shared data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Flow of dat from to To category Ids\"\"\"\n",
    "sns.set(rc={'figure.figsize':(18,10)})\n",
    "a = df_fromCat_toCat.groupby(['FromCat_label', 'ToCat_label']).count()\n",
    "agg_data = a.reset_index()\n",
    "ha = agg_data[['FromCat_label', 'ToCat_label', 'Count']]\n",
    "ha = ha.pivot('FromCat_label', 'ToCat_label', 'Count')\n",
    "ha.fillna(0)\n",
    "sns.heatmap(ha, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a)(ii)(5) Category v. Category (Correlation heatmap and dependency analysis)\n",
    "\n",
    "For understanding the relation between the various categories, we have plotted a correlation graph. Since our columns are categorical we are using chi-squared independence test to get a metric of dependency between the columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_p_values = []\n",
    "for compare_column in df_prcbreaches_temp.columns.tolist():\n",
    "    c_data = [] \n",
    "    for compare_with in df_prcbreaches_temp.columns.tolist():\n",
    "        X = df_prcbreaches_temp[compare_column].astype(str)\n",
    "        Y = df_prcbreaches_temp[compare_with].astype(str)\n",
    "        X.name = X.name + \"__\"\n",
    "        df_observed = pd.crosstab(Y,X) \n",
    "        chi2, p, dof, expected = stats.chi2_contingency(df_observed.values)\n",
    "        c_data.append(p)\n",
    "    collection_p_values.append(c_data)\n",
    "p_values = pd.DataFrame(collection_p_values, columns=df_prcbreaches_temp.columns.tolist(), index=df_prcbreaches_temp.columns.tolist())\n",
    "\n",
    "\n",
    "p_values.index = p_values.columns\n",
    "log_values = p_values.apply(lambda x: np.log(x)*-1)\n",
    "\n",
    "mask = np.zeros_like(log_values, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "log_values.replace(np.inf, 10000, inplace=True)\n",
    "sns.heatmap(log_values, mask=mask, cmap=cmap, vmax=1000, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis** : The square with higher color intensity in the above graph show higher correlation between the columns. The `OrgID` column seems to be perfectly random and the Chi square test seems to reflect the randomness with minimum intensity and we can infer that the column is independent of any other columns. The column `Source.of.Breach.Notification` seems to be highly correlated with the columns `CatID`, `Region`, `Entity_prc`, `Location`, `Year` and `Type`. Similarly, we can observe that `Location` seems to be highly correlated with `CatId`, `Region`, `Source.of.Breach.Notification`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6(b). Data Prediction and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6(b)(1). Linear Regression Model\n",
    "\n",
    "We have data for number of breaches for a certain year. One of the possible things that we can do with that data is to predict the number of breaches for next upcoming years. We can train a regression model and use that regression model to predict the number of breaches for next year. \n",
    "\n",
    "In the following section we will train a regression model and try to predict the number of breach for the upcoming years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_counts = df_prcbreaches_temp.groupby('Year').count()\n",
    "record_counts['counts'] = record_counts['Name']\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "sns.regplot(x=record_counts.index.tolist(), y=record_counts['counts'], marker='+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "\n",
    "The above graph is a regression plot for the data set we have. On the x-axis are the years while on the y-axis are the counts of breaches. There is also a linear regression line that can be used to predict the future values. The following code breaks the plot into a usable model and we will use the model to predict new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept , r_value ,p_value, std_err = stats.linregress(x=record_counts.index.tolist(), y=record_counts['counts'])\n",
    "plt.plot(record_counts.index.tolist(), record_counts['counts'],'*',label = 'line')\n",
    "plt.plot(record_counts.index.tolist(), (intercept+slope*record_counts.index).tolist(),'r',label='regression line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "\n",
    "In the above code, we have `slope_Cal, intercept_Cal , r_value_Cal ,p_value_Cal, std_err_Cal` variables that represent our linear regression model. The important variables are the `slope_Cal` and `intercept_Cal`, using these two we can predict the number of breaches for the coming years. Let's predict the number of breach for the year `2020`.\n",
    "\n",
    "Of course, this prediction does not take into consideration any drastric changes that may happen to our state or federal privacy laws. The estimated number of breaches (approximately 700) that our model predicts for the year 2020 is congruent with the numbers we have seen over the past few years. However, CA recenlty passed a privacy law that is expected to come into effect in 2020. CA can adopt stricter privacy guidelines for health care data than HIPAA offers so long as the state law does not preempt the federal law. \n",
    "\n",
    "Aside from the changes we may see in the futre in terms of the updates to CA state law, privacy rights groups are advocating for congress to pass a federal privacy law. If this law passes in the near future, it is hard to tell how it will influence the results we are predicting here. However, we can assume that, if the change further expands liability to third partties the same way HITECH did, we will see an increase of reported breaches in the future in the same way we saw it after 2014. There is also the possibility that healthcare organizations adopt better technologies, improve employee training and conduct periodic risk assesments, which it can all help reduce the number of data breaches we see today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_year = 2020\n",
    "num_of_breaches = intercept + slope * pred_year\n",
    "print (\"The number of breaches for year\", pred_year, \"could be\", num_of_breaches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "We can also use similar regression model to predict the number of breaches for certain states. We will use the code below to observe `California` state and try to predict number of breaches for this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prcbreaches_temp.describe()\n",
    "state_dist = df_prcbreaches_temp[df_prcbreaches_temp['State'] == \"California\"].groupby('Year').count()\n",
    "slope_Cal, intercept_Cal , r_value_Cal ,p_value_Cal, std_err_Cal = stats.linregress(x=state_dist.index.tolist(), y=state_dist['Name'])\n",
    "plt.plot(state_dist.index.tolist(), state_dist['Name'],'*', label = 'line')\n",
    "plt.plot(state_dist.index.tolist(), (intercept_Cal+slope_Cal*state_dist.index).tolist(),'r',label='regression line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_year = 2020\n",
    "num_of_breaches = intercept_Cal + slope_Cal * pred_year\n",
    "print (\"The number of breaches for year\", pred_year, \"in California could be\", num_of_breaches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6(b)(2). Decision Tree\n",
    "\n",
    "Base on the correlation heatmap we want to predict the type of breach based on the other fields. We will be using the Decision Tree model to train and predict based on our dataset. But before we start we want to convert our categorical data to numerical ones using the `LabelEncoder` because the decision tree model in the scipy library doesn't deal with the categorical model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predicting region based on the X values\n",
    "X= df_prcbreaches_temp[[\"Category_Name\",'Source.of.Breach.Notification','Year', 'State' ]] #,'Source.of.Breach.Notification'\n",
    "# Y_Type =  df_prcbreaches_temp[[\"Category_Name\",'Source.of.Breach.Notification','Year','Type' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = df_prcbreaches_temp['Type']\n",
    "y = df_prcbreaches_temp['Type']\n",
    "le_for_y = LabelEncoder()\n",
    "le_for_y.fit(y)\n",
    "y = le_for_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regions\n",
    "le_for_y.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFloatTransformationforColumns(X):\n",
    "    le_list = []\n",
    "    for item in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(X[item])\n",
    "        X[item] = le.transform(X[item])\n",
    "        le_list.append(le)\n",
    "    return le_list\n",
    "\n",
    "label_lst_result = getFloatTransformationforColumns(X)\n",
    "#label_lst_result_src = getFloatTransformationforColumns(X_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very imp\n",
    "for index, label_encoder in enumerate(label_lst_result):\n",
    "    print (X.columns[index], list(enumerate(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "\n",
    "Now that our encoding is done, we will save the encoding schema to use it later to decode the decision tree model. Finally, we will train our model to do the prediction. We will start by breaking data into training and testing set and then fitting it to our model. We will use the trained model to predict and calculate accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test= train_test_split(X, y, test_size=0.30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(min_samples_split=50)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "# min_samples_split=800\n",
    "# predict\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "\n",
    "The decision tree model we used has an accuracy of `44.99%` which is less than ideal to be used in real-world scenarios, but we might be able to find better accuracy if we are able to expand our data set or perhaps use other models. But before trying with another model let's see what our decision tree looks like actually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "# we are minimizing the depth of decision tree by specifying higher min_sample_split\n",
    "dt_classifier = DecisionTreeClassifier(min_samples_split=400)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(dt_classifier, out_file=dot_data,\n",
    "                 filled=True, rounded=True, feature_names=X.columns, \n",
    "class_names=le_for_y.classes_, special_characters=True)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph[0].create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "\n",
    "The decision tree model turned out categorical data set to a numerical data set and used it as continious data. However we can decode the data using the `LabelEncoder` schema we had saved previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6(b)(3). KNN Model\n",
    "\n",
    "Decision Trees performed poorly for the data set above, we now try to use a new model for the prediction. We will repeat same procedure for the new model as we did for the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=40)\n",
    "model.fit(X_train, y_train) \n",
    "# Predict the labels for X_test => y_predict\n",
    "# accuracy is comparison between true value and predictions\n",
    "# y_test\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "\n",
    "Accuracy score for the KNN is simillar to the one with the decision tree model. It seems like Decision tree works better for the purpose compared to the KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6(b)(4). Random Forest\n",
    "\n",
    "We can again try to find better accuracy with a Random Forest model. We fit a simple model for our data set to observe how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(min_samples_split=50)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_predict = rf_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "\n",
    "Random Forest classifier seems to be better than decision tree and K nearest neighbour with slightly increase in accuracy. But the overall accuracy is still below 50%, one of the reason might be the number of possible values that we should predict since `Type` field has 8 unique categories predicting them accurately would be difficult compared to a binary categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting `State` based on `Category_Name`, `Year`, `Source.of.Breach.Notification` and `Type`\n",
    "\n",
    "We can predict one of the attribute based on other attributes using our models. Can we accurately predict the state based on other attributes? If we can do so we can use the model for the future analysis of the problem. We initially noticed that there were many variables with missing values so we might be able to get those missing values using our model. Also, it would be more interesting to see whether category_name, year, source and type of breach can infer the location of that organization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using the Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = df_prcbreaches_temp[[\"Category_Name\",'Source.of.Breach.Notification','Year', 'Type' ]] #,'Source.of.Breach.Notification'\n",
    "y_state = df_prcbreaches_temp['State']\n",
    "le_for_y_state = LabelEncoder()\n",
    "le_for_y_state.fit(y_state)\n",
    "y_state = le_for_y_state.transform(y_state)\n",
    "\n",
    "label_lst_result_state = getFloatTransformationforColumns(X_state)\n",
    "\n",
    "X_train_state, X_test_state , y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.30, shuffle=True)\n",
    "\n",
    "dt_classifier_state = DecisionTreeClassifier(min_samples_split=50)\n",
    "dt_classifier_state.fit(X_train_state, y_train_state)\n",
    "# min_samples_split=800\n",
    "# predict\n",
    "\n",
    "\n",
    "y_pred_state = dt_classifier_state.predict(X_test_state)\n",
    "print('Accuracy : ', accuracy_score(y_test_state, y_pred_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted accuracy for State here is approx. ***20%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_state = KNeighborsClassifier(n_neighbors=40)\n",
    "model_state.fit(X_train_state, y_train_state) \n",
    "y_predict_state = model_state.predict(X_test_state)\n",
    "print(accuracy_score(y_test_state, y_predict_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted accuracy for State here is approx. ***21%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model_state = RandomForestClassifier(min_samples_split=50)\n",
    "rf_model_state.fit(X_train_state, y_train_state)\n",
    "y_predict_state = rf_model_state.predict(X_test_state)\n",
    "print(accuracy_score(y_test_state, y_predict_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted accuracy for State here is again approx. ***20%***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis** Prediction of state seems to have lower accuracy than prediction of the type of breach. We can look at the number of unique class the state column has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_prcbreaches_temp['State'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "\n",
    "The state column has `66` separate classes, so one of the possible reasons for why our model performed bad predicting state would be due to large set of distinct classes. Also, we can use more sophisticated model to predict the result based on our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Challenges \n",
    "\n",
    "- Incomplete data set with many empty cells \n",
    "- Numeric vs. categorical values \n",
    "- Understanding how the law applies (state and federal levels) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Future work \n",
    "\n",
    "There are many ways in which this data set could be used in the future. For instance, one thing we could do would be to integrate the data we have already cleaned with other set available on thedatamap.org website regarding health mobile devices. \n",
    "\n",
    "Additionally, with this information we could also predict other factors we did not get a chance to look at. For instance, in addition to predicting the number of breaches in 2020 and learning more about what roles reporting agencies will have in the future, we could look in depth at the type of breaches that may be most prevalent, the location of those breaches. \n",
    "\n",
    "Because this data set is so dependent on the changes in our privacy laws, there will be many future applications for the information we have uncovered. \n",
    "\n",
    "Also, with the potential modifications of privacy laws in the US we can make better analysis on how these modifications have brought better impact in curbing the privacy breaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANK YOU FOR A GREAT SEMESTER! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
